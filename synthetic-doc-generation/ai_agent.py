"""
Document-Based AI Agent (PDF-first RAG) using Pydantic AI

Assumptions
- Synthetic PDFs (e.g., invoices, bank statements) are ALREADY generated by ydata-sdk
  and placed under INPUT_PDF_DIR. This script does NOT generate documents.

What this script does
1) Load PDFs and extract text
2) Chunk -> embed -> FAISS index
3) Define a simple RAG tool
4) Build an AI Agent with Pydantic AI that uses the RAG tool to answer questions

Install (example)
  pip install pypdf sentence-transformers faiss-cpu langchain pydantic-ai openai tiktoken

Notes
- Set OPENAI_API_KEY in your env if you want model-backed answers.
- The agent uses Pydantic AI; if pydantic_ai is unavailable, a simple non-agent fallback runs.
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Optional

# ---------------------------
# Config
# ---------------------------

INPUT_PDF_DIR = Path("synthetic_pdfs")  # <-- put your ydata-sdk generated PDFs here
ARTIFACTS_DIR = Path("artifacts")
ARTIFACTS_DIR.mkdir(exist_ok=True)

INDEX_PATH = ARTIFACTS_DIR / "chunks.index"
DOCS_JSONL = ARTIFACTS_DIR / "documents.jsonl"
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

CHUNK_SIZE = 900
CHUNK_OVERLAP = 120

MODEL_NAME = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# ---------------------------
# Imports
# ---------------------------

# PDF parsing
from pypdf import PdfReader

# Embeddings & Vector DB
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

# Chunking
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Optional model client for Pydantic AI (OpenAI)
try:
    from pydantic_ai import Agent, RunContext
    from pydantic_ai.models.openai import OpenAIModel
    from pydantic import BaseModel
    PAI_OK = True
except Exception:
    PAI_OK = False

# ---------------------------
# 0) Ensure PDFs exist (placeholder for ydata-sdk)
# ---------------------------

def ensure_pdfs_present(pdf_dir: Path) -> List[Path]:
    """
    ydata-sdk is responsible for generating PDFs and placing them in `pdf_dir`.
    Expected: one or more .pdf files inside `pdf_dir`.
    """
    if not pdf_dir.exists():
        raise FileNotFoundError(
            f"{pdf_dir.resolve()} not found. Generate PDFs with ydata-sdk and place them here."
        )
    pdfs = sorted([p for p in pdf_dir.glob("**/*.pdf") if p.is_file()])
    if not pdfs:
        raise FileNotFoundError(
            f"No PDFs found in {pdf_dir.resolve()}. "
            "Generate them with ydata-sdk and re-run."
        )
    return pdfs

# ---------------------------
# 1) Read PDFs -> Extract Text
# ---------------------------

def extract_text_from_pdf(pdf_path: Path) -> str:
    reader = PdfReader(str(pdf_path))
    texts = []
    for page in reader.pages:
        try:
            texts.append(page.extract_text() or "")
        except Exception:
            texts.append("")
    return "\n".join(texts).strip()

def load_corpus_from_pdfs(pdfs: List[Path]) -> List[Dict]:
    corpus = []
    for p in pdfs:
        text = extract_text_from_pdf(p)
        corpus.append({"path": str(p), "text": text})
    return corpus

# ---------------------------
# 2) Chunking
# ---------------------------

def chunk_documents(corpus: List[Dict], chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP) -> List[Dict]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", " ", ""],
    )
    chunks = []
    for doc in corpus:
        parts = splitter.split_text(doc["text"]) if doc["text"] else []
        if not parts:
            parts = ["[EMPTY TEXT EXTRACTED]"]
        for i, ch in enumerate(parts):
            chunks.append(
                {"source": doc["path"], "chunk_id": f"{doc['path']}::chunk_{i}", "text": ch}
            )
    return chunks

# ---------------------------
# 3) Embeddings & FAISS (cosine via inner product on L2-normalized vectors)
# ---------------------------

class VectorIndex:
    def __init__(self, model_name: str = EMBED_MODEL):
        self.model_name = model_name
        self.model = SentenceTransformer(model_name)
        self.index: Optional[faiss.Index] = None
        self.dim: Optional[int] = None

    @staticmethod
    def _l2_normalize(x: np.ndarray) -> np.ndarray:
        norms = np.linalg.norm(x, axis=1, keepdims=True)
        norms[norms == 0] = 1.0
        return x / norms

    def build(self, documents: List[Dict], index_path: Path, docs_path: Path):
        print(f"[emb] encoding {len(documents)} chunks with {self.model_name}...")
        emb = self.model.encode([d["text"] for d in documents], convert_to_numpy=True, show_progress_bar=True)
        self.dim = emb.shape[1]
        emb = self._l2_normalize(emb).astype("float32")

        print("[faiss] building inner-product index (cosine on normalized embeddings)...")
        self.index = faiss.IndexFlatIP(self.dim)
        self.index.add(emb)

        print(f"[save] index -> {index_path}")
        faiss.write_index(self.index, str(index_path))

        print(f"[save] documents -> {docs_path}")
        with open(docs_path, "w", encoding="utf-8") as f:
            for d in documents:
                f.write(json.dumps(d, ensure_ascii=False) + "\n")

    def load(self, index_path: Path, docs_path: Path) -> List[Dict]:
        self.index = faiss.read_index(str(index_path))
        with open(docs_path, "r", encoding="utf-8") as f:
            docs = [json.loads(line) for line in f]
        if self.dim is None:
            self.dim = self.model.encode(["_probe_"], convert_to_numpy=True).shape[1]
        return docs

    def search(self, query: str, docs: List[Dict], k: int = 5) -> List[Dict]:
        q_vec = self.model.encode([query], convert_to_numpy=True)
        q_vec = q_vec / max(np.linalg.norm(q_vec), 1e-12)
        D, I = self.index.search(q_vec.astype("float32"), k)
        out = []
        for rank, idx_i in enumerate(I[0]):
            if idx_i < 0:
                continue
            d = docs[idx_i]
            out.append(
                {"rank": rank + 1, "score": float(D[0][rank]), "source": d["source"], "text": d["text"]}
            )
        return out

# ---------------------------
# 4) Pydantic AI Agent with a RAG Tool
# ---------------------------

SYSTEM_PROMPT = (
    "You are a helpful assistant for document-based Q&A.\n"
    "Use ONLY the provided retrieval tool to ground your answers in the documents.\n"
    "If the answer is not in the retrieved context, say you don't know.\n"
    "When possible, cite the source filenames."
)

# Pydantic model for retrieved chunk(s)
if PAI_OK:
    class RetrievedChunk(BaseModel):
        source: str
        score: float
        text: str
    
    class RAGDeps(BaseModel):
        """Dependencies for RAG agent - holds vector index and documents"""
        vector_index: VectorIndex
        docs: List[Dict]
        
        class Config:
            arbitrary_types_allowed = True

    # Define the agent; we'll attach a tool for retrieval
    model = OpenAIModel(MODEL_NAME) 
    agent = Agent(
        model=model,
        deps_type=RAGDeps,
        system_prompt=SYSTEM_PROMPT,
    )
    
    @agent.tool
    def retrieve(ctx: RunContext[RAGDeps], query: str, k: int = 5) -> List[RetrievedChunk]:
        """
        Retrieve top-k document chunks relevant to `query`.
        """
        results = ctx.deps.vector_index.search(query, ctx.deps.docs, k=k)
        return [RetrievedChunk(source=r["source"], score=r["score"], text=r["text"]) for r in results]

# ---------------------------
# 5) Fallback (non-agent) answerer
# ---------------------------

def non_agent_answer(query: str, vindex: VectorIndex, docs: List[Dict], k: int = 5) -> str:
    ctx = vindex.search(query, docs, k=k)
    if not ctx:
        return "I don't know."
    from pathlib import Path as _Path
    top = ctx[:2]
    out = ["[Non-agent fallback] Relevant excerpts:"]
    for t in top:
        out.append(f"- {_Path(t['source']).name} (score={t['score']:.3f}):\n{t['text'][:900]}")
    return "\n\n".join(out)

# ---------------------------
# 6) Main
# ---------------------------

def main():
    # Ensure PDFs exist (generated by ydata-sdk elsewhere)
    pdfs = ensure_pdfs_present(INPUT_PDF_DIR)
    print(f"[pdf] found {len(pdfs)} PDFs under {INPUT_PDF_DIR.resolve()}")

    # Load & chunk
    corpus = load_corpus_from_pdfs(pdfs)
    chunks = chunk_documents(corpus)
    print(f"[chunk] produced {len(chunks)} chunks")

    # Build/Save index
    vindex = VectorIndex(EMBED_MODEL)
    vindex.build(chunks, INDEX_PATH, DOCS_JSONL)

    # Load back & run samples
    docs_loaded = vindex.load(INDEX_PATH, DOCS_JSONL)
    print(f"[index] loaded {len(docs_loaded)} chunks from disk")

    queries = [
        "Which vendors appear on the invoices?",
        "What invoice has the highest total?",
        "List any transactions with negative amounts.",
        "Which bank issued the statements?",
        "What is the invoice date for INV-1003?",
    ]

    print("=" * 100)
    if PAI_OK:
        # Run via Pydantic AI Agent
        print("[agent] Using Pydantic AI agent with OpenAI model")
        
        # Create dependencies
        deps = RAGDeps(vector_index=vindex, docs=docs_loaded)
        
        for q in queries:
            print("-" * 100)
            print("Q:", q)
            # Prompt instructs the model to call our `retrieve` tool before answering
            prompt = (
                "Use the `retrieve` tool to fetch relevant context, then answer.\n"
                f"Question: {q}\n"
                "If the answer isn't in the documents, say you don't know."
            )
    
            result = agent.run_sync(prompt, deps=deps)
            print("A:", result.output)   # result.data is the final text answer
    else:
        # Fallback non-agent path (when pydantic_ai isn't installed)
        print("[fallback] pydantic-ai not installed; using non-agent retrieval-only answers.")
        for q in queries:
            print("-" * 100)
            print("Q:", q)
            print("A:", non_agent_answer(q, vindex, docs_loaded))

if __name__ == "__main__":
    main()
